import datetime
import uuid
from typing import Any, Dict, List, Optional

import streamlit as st
from langchain.callbacks.streamlit import StreamlitCallbackHandler
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import AIMessage, HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from constant import UI_LABELS

# ì–¸ì–´ë³„ ì¶”ì²œ ì§ˆë¬¸
suggested_questions_map = {
    "ko": [
        "ì´ ì˜ìƒì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•µì‹¬ ê°œë…ì„ ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
        "ì‹¤ìƒí™œì— ì ìš©í•  ìˆ˜ ìˆëŠ” ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ],
    "en": [
        "What are the main points discussed in this video?",
        "Can you explain the key concepts in more detail?",
        "What are the practical takeaways from this video?",
    ],
    "ja": [
        "ã“ã®å‹•ç”»ã®ä¸»ãªå†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "é‡è¦ãªæ¦‚å¿µã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "å®Ÿç”Ÿæ´»ã«ã©ã®ã‚ˆã†ã«å¿œç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
    ],
    "zh": [
        "æœ¬è§†é¢‘çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "è¯·ç®€å•è§£é‡Šä¸€ä¸‹æ ¸å¿ƒæ¦‚å¿µã€‚",
        "æœ‰å“ªäº›å®é™…åº”ç”¨ï¼Ÿ",
    ],
    "fr": [
        "Quels sont les points principaux abordÃ©s dans cette vidÃ©o ?",
        "Pouvez-vous expliquer les concepts clÃ©s plus en dÃ©tail ?",
        "Quelles sont les applications pratiques de cette vidÃ©o ?",
    ],
    "de": [
        "Was sind die Hauptpunkte dieses Videos?",
        "KÃ¶nnen Sie die wichtigsten Konzepte nÃ¤her erlÃ¤utern?",
        "Was sind die praktischen Erkenntnisse aus diesem Video?",
    ],
    "es": [
        "Â¿CuÃ¡les son los puntos principales de este video?",
        "Â¿Puede explicar los conceptos clave con mÃ¡s detalle?",
        "Â¿CuÃ¡les son las aplicaciones prÃ¡cticas de este video?",
    ],
}

# ì–¸ì–´ë³„ ì´ˆê¸° assistant ë©”ì‹œì§€
initial_assistant_message_map = {
    "ko": lambda video_title: f"""ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! "{video_title}" ì˜ìƒì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì˜ìƒì˜ ë‚´ìš©, ìš”ì•½, ëŒ€ë³¸ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ í•´ë³´ì„¸ìš”:

â€¢ í•µì‹¬ ê°œë… ë° ì£¼ìš” ë‚´ìš©
â€¢ êµ¬ì²´ì  ì„¸ë¶€ì‚¬í•­ì´ë‚˜ ì˜ˆì‹œ
â€¢ ë³µì¡í•œ ì£¼ì œì— ëŒ€í•œ ì„¤ëª…
â€¢ ì‹¤ìƒí™œ ì ìš© ë°©ë²•
â€¢ ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ì¸ì‚¬ì´íŠ¸

ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?""",
    "en": lambda video_title: f"""ğŸ‘‹ Hello! I'm your AI assistant for "{video_title}". I've analyzed the video content, summary, and transcript. Feel free to ask me anything about:

â€¢ Key concepts and main points
â€¢ Specific details or examples
â€¢ Clarifications on complex topics
â€¢ Practical applications
â€¢ Related questions or insights

What would you like to know?""",
    "ja": lambda video_title: f"""ğŸ‘‹ ã“ã‚“ã«ã¡ã¯ï¼ã€Œ{video_title}ã€ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å‹•ç”»ã®å†…å®¹ã€è¦ç´„ã€å­—å¹•ã‚’åˆ†æã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªè³ªå•ã‚’è‡ªç”±ã«ã©ã†ãï¼š

â€¢ é‡è¦ãªæ¦‚å¿µã‚„ä¸»ãªãƒã‚¤ãƒ³ãƒˆ
â€¢ å…·ä½“çš„ãªè©³ç´°ã‚„ä¾‹
â€¢ è¤‡é›‘ãªãƒˆãƒ”ãƒƒã‚¯ã®èª¬æ˜
â€¢ å®Ÿç”Ÿæ´»ã§ã®å¿œç”¨æ–¹æ³•
â€¢ é–¢é€£ã™ã‚‹è³ªå•ã‚„æ´å¯Ÿ

ä½•ãŒçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ""",
    "zh": lambda video_title: f"""ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯â€œ{video_title}â€è§†é¢‘çš„AIåŠ©æ‰‹ã€‚æˆ‘å·²åˆ†æäº†è§†é¢‘å†…å®¹ã€æ‘˜è¦å’Œå­—å¹•ã€‚æ¬¢è¿éšæ—¶æé—®ï¼š

â€¢ å…³é”®æ¦‚å¿µå’Œä¸»è¦å†…å®¹
â€¢ å…·ä½“ç»†èŠ‚æˆ–ç¤ºä¾‹
â€¢ å¤æ‚ä¸»é¢˜çš„è§£é‡Š
â€¢ å®é™…åº”ç”¨
â€¢ ç›¸å…³é—®é¢˜æˆ–è§è§£

ä½ æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ""",
    "fr": lambda video_title: f"""ğŸ‘‹ Bonjour ! Je suis votre assistant IA pour "{video_title}". J'ai analysÃ© le contenu, le rÃ©sumÃ© et la transcription de la vidÃ©o. N'hÃ©sitez pas Ã  poser des questions sur :

â€¢ Concepts clÃ©s et points principaux
â€¢ DÃ©tails spÃ©cifiques ou exemples
â€¢ Explications sur des sujets complexes
â€¢ Applications pratiques
â€¢ Questions ou idÃ©es connexes

Que souhaitez-vous savoir ?""",
    "de": lambda video_title: f"""ğŸ‘‹ Hallo! Ich bin dein KI-Assistent fÃ¼r "{video_title}". Ich habe den Videoinhalt, die Zusammenfassung und das Transkript analysiert. Stelle gerne Fragen zu:

â€¢ Zentrale Konzepte und Hauptpunkte
â€¢ Spezifische Details oder Beispiele
â€¢ ErklÃ¤rungen zu komplexen Themen
â€¢ Praktische Anwendungen
â€¢ Verwandte Fragen oder Erkenntnisse

Was mÃ¶chtest du wissen?""",
    "es": lambda video_title: f"""ğŸ‘‹ Â¡Hola! Soy tu asistente de IA para "{video_title}". He analizado el contenido, el resumen y la transcripciÃ³n del video. SiÃ©ntete libre de preguntar sobre:

â€¢ Conceptos clave y puntos principales
â€¢ Detalles especÃ­ficos o ejemplos
â€¢ Aclaraciones sobre temas complejos
â€¢ Aplicaciones prÃ¡cticas
â€¢ Preguntas o ideas relacionadas

Â¿QuÃ© te gustarÃ­a saber?""",
}

LABELS = UI_LABELS.get(st.session_state.get("selected_lang", "ko"), UI_LABELS["ko"])

# Ensure LABELS is defined at the top of the file for global usage.


class LangChainChatManager:
    def __init__(self):
        self.memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
        self.chat_template = self._create_chat_template()

    # ì–¸ì–´ë³„ system í”„ë¡¬í”„íŠ¸ ë°˜í™˜ í•¨ìˆ˜
    def get_chat_system_prompt(self, lang_code):
        if lang_code == "ko":
            return """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ìš”ì•½ê³¼ ëŒ€ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì˜ìƒ ìš”ì•½:
{summary}

ì›ë³¸ ëŒ€ë³¸ ì¼ë¶€:
{transcript}

ìœ„ ì˜ìƒ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ì˜ìƒ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
ì§ˆë¬¸ì´ ì˜ìƒì— ì—†ëŠ” ì •ë³´ë¼ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ë°íˆê³ , ì˜ìƒ ë²”ìœ„ ë‚´ì—ì„œ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”."""
        elif lang_code == "en":
            return """You are an AI assistant that answers questions based on a YouTube video summary and transcript.

Video summary:
{summary}

Part of the original transcript:
{transcript}

Refer to the above video content and answer the user's questions kindly and specifically.
Your answer must be written in English, and provide accurate information related to the video content.
If the question is about information not present in the video, clearly state that fact and provide any helpful information within the scope of the video content."""
        elif lang_code == "ja":
            return """ã‚ãªãŸã¯YouTubeå‹•ç”»ã®è¦ç´„ã¨å­—å¹•ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

å‹•ç”»ã®è¦ç´„:
{summary}

å…ƒã®å­—å¹•ã®ä¸€éƒ¨:
{transcript}

ä¸Šè¨˜ã®å‹•ç”»å†…å®¹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«è¦ªåˆ‡ã‹ã¤å…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚
å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã€å‹•ç”»å†…å®¹ã«é–¢é€£ã™ã‚‹æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
è³ªå•ãŒå‹•ç”»ã«ãªã„æƒ…å ±ã®å ´åˆã¯ã€ãã®äº‹å®Ÿã‚’æ˜ç¢ºã«ä¼ãˆã€å‹•ç”»ã®ç¯„å›²å†…ã§å½¹ç«‹ã¤æƒ…å ±ã‚’æ¡ˆå†…ã—ã¦ãã ã•ã„ã€‚"""
        elif lang_code == "zh":
            return """ä½ æ˜¯ä¸€ä¸ªåŸºäºYouTubeè§†é¢‘æ‘˜è¦å’Œå­—å¹•å›ç­”é—®é¢˜çš„AIåŠ©æ‰‹ã€‚

è§†é¢‘æ‘˜è¦:
{summary}

åŸå§‹å­—å¹•ç‰‡æ®µ:
{transcript}

è¯·å‚è€ƒä¸Šè¿°è§†é¢‘å†…å®¹ï¼Œå‹å¥½ä¸”å…·ä½“åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¿…é¡»ç”¨ä¸­æ–‡ä½œç­”ï¼Œå¹¶æä¾›ä¸è§†é¢‘å†…å®¹ç›¸å…³çš„å‡†ç¡®ä¿¡æ¯ã€‚
å¦‚æœé—®é¢˜æ¶‰åŠè§†é¢‘ä¸­æ²¡æœ‰çš„ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ï¼Œå¹¶åœ¨è§†é¢‘å†…å®¹èŒƒå›´å†…æä¾›æœ‰ç”¨çš„ä¿¡æ¯ã€‚"""
        elif lang_code == "fr":
            return """Vous Ãªtes un assistant IA qui rÃ©pond aux questions Ã  partir du rÃ©sumÃ© et de la transcription d'une vidÃ©o YouTube.

RÃ©sumÃ© de la vidÃ©o :
{summary}

Extrait de la transcription originale :
{transcript}

RÃ©fÃ©rez-vous au contenu ci-dessus et rÃ©pondez de maniÃ¨re prÃ©cise et bienveillante.
Votre rÃ©ponse doit Ãªtre rÃ©digÃ©e en FranÃ§ais et fournir des informations exactes liÃ©es Ã  la vidÃ©o.
Si la question concerne une information absente de la vidÃ©o, indiquez-le clairement et fournissez toute information utile dans le cadre du contenu vidÃ©o."""
        elif lang_code == "de":
            return """Du bist ein KI-Assistent, der Fragen auf Basis einer YouTube-Videozusammenfassung und eines Transkripts beantwortet.

Videozusammenfassung:
{summary}

Teil des Originaltranskripts:
{transcript}

Beziehe dich auf den obigen Videoinhalt und beantworte die Fragen freundlich und konkret.
Deine Antwort muss auf Deutsch verfasst sein und genaue, zum Video passende Informationen liefern.
Falls die Frage Informationen betrifft, die nicht im Video enthalten sind, weise klar darauf hin und gib hilfreiche Hinweise im Rahmen des Videoinhalts."""
        elif lang_code == "es":
            return """Eres un asistente de IA que responde preguntas basadas en el resumen y la transcripciÃ³n de un video de YouTube.

Resumen del video:
{summary}

Parte de la transcripciÃ³n original:
{transcript}

Consulta el contenido anterior y responde amablemente y de manera especÃ­fica.
Tu respuesta debe estar escrita en EspaÃ±ol y proporcionar informaciÃ³n precisa relacionada con el video.
Si la pregunta trata sobre informaciÃ³n no presente en el video, indÃ­calo claramente y proporciona cualquier informaciÃ³n Ãºtil dentro del alcance del contenido del video."""
        else:
            # ê¸°ë³¸ê°’: ì˜ì–´
            return """You are an AI assistant that answers questions based on a YouTube video summary and transcript.

Video summary:
{summary}

Part of the original transcript:
{transcript}

Refer to the above video content and answer the user's questions kindly and specifically.
Your answer must be written in English, and provide accurate information related to the video content.
If the question is about information not present in the video, clearly state that fact and provide any helpful information within the scope of the video content."""

    def _create_chat_template(self) -> ChatPromptTemplate:
        """ì±„íŒ…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        selected_lang = st.session_state.get("selected_lang", "ko")
        system_prompt = self.get_chat_system_prompt(selected_lang)
        template = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{question}"),
            ]
        )
        return template

    def get_llm_model(self, model_id: str, provider: str, api_key: str):
        """ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if provider == "gemini" or "gemini" in model_id:
            return ChatGoogleGenerativeAI(
                model=model_id,
                google_api_key=api_key,
                temperature=0.2,
                convert_system_message_to_human=True,
            )
        elif provider == "openai" or "gpt" in model_id:
            return ChatOpenAI(model=model_id, openai_api_key=api_key, temperature=0.2)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì œê³µì: {provider}")

    def create_chain(self, llm):
        """LangChain ì²´ì¸ ìƒì„±"""
        from langchain.chains import ConversationChain

        chain = ConversationChain(
            llm=llm, memory=self.memory, prompt=self.chat_template, verbose=True
        )
        return chain

    def get_response(
        self,
        question: str,
        summary: str,
        transcript: str,
        model_id: str,
        provider: str,
        api_key: str,
    ) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        try:
            llm = self.get_llm_model(model_id, provider, api_key)

            # ì§ì ‘ invoke ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            formatted_prompt = self.chat_template.format_messages(
                summary=summary,
                transcript=transcript[:2000],
                question=question,
                chat_history=self.memory.chat_memory.messages,
            )

            response = llm.invoke(formatted_prompt)

            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
            self.memory.save_context({"question": question}, {"answer": response.content})

            return response.content

        except Exception as e:
            return f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def get_streaming_response(
        self,
        question: str,
        summary: str,
        transcript: str,
        model_id: str,
        provider: str,
        api_key: str,
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ê¸°"""
        try:
            llm = self.get_llm_model(model_id, provider, api_key)

            formatted_prompt = self.chat_template.format_messages(
                summary=summary,
                transcript=transcript[:2000],
                question=question,
                chat_history=self.memory.chat_memory.messages,
            )

            response_content = ""
            for chunk in llm.stream(formatted_prompt):
                if hasattr(chunk, "content"):
                    response_content += chunk.content
                    yield chunk.content

            # ì™„ë£Œëœ ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            self.memory.save_context({"question": question}, {"answer": response_content})

        except Exception as e:
            yield f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def clear_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.memory.clear()


def render_chat_tab():
    """ì±„íŒ… íƒ­ ë Œë”ë§ ë©”ì¸ í•¨ìˆ˜"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])

    st.subheader(LABELS["chat_tab"])

    # LangChain ì±„íŒ… ë§¤ë‹ˆì € ì´ˆê¸°í™”
    if "langchain_chat_manager" not in st.session_state:
        st.session_state.langchain_chat_manager = LangChainChatManager()

    chat_manager = st.session_state.langchain_chat_manager

    # ì±„íŒ… ìƒíƒœ ì´ˆê¸°í™”
    _initialize_chat_state()

    # ì–¸ì–´ ë° ì˜ìƒ ì œëª© ì •ë³´
    selected_lang = st.session_state.get("selected_lang", "ko")
    video_title = st.session_state.get("video_title", "ìœ íŠœë¸Œ ì˜ìƒ")
    # ì˜ìƒ ì œëª©ì´ ì—†ìœ¼ë©´ video_idë¡œ ëŒ€ì²´
    if not video_title and st.session_state.get("video_id"):
        video_title = st.session_state["video_id"]

    # ì´ˆê¸° assistant ë©”ì‹œì§€ ë° ì¶”ì²œ ì§ˆë¬¸
    initial_msg_func = initial_assistant_message_map.get(
        selected_lang, initial_assistant_message_map["en"]
    )
    initial_msg = initial_msg_func(video_title)
    suggested_questions = suggested_questions_map.get(selected_lang, suggested_questions_map["en"])

    # ìµœì´ˆ assistant ë©”ì‹œì§€ êµì²´ (ìµœì´ˆ 1íšŒë§Œ)
    if st.session_state.chat_messages and st.session_state.chat_messages[0]["id"] == "welcome":
        st.session_state.chat_messages[0]["content"] = initial_msg

    # ì±„íŒ… ì„¤ì • ì˜µì…˜
    _render_chat_settings()

    # ì¶”ì²œ ì§ˆë¬¸ ë²„íŠ¼ UI
    st.markdown(f"**{LABELS['suggested_questions']}**")
    q_cols = st.columns(len(suggested_questions))
    for idx, q in enumerate(suggested_questions):
        if q_cols[idx].button(q, key=f"suggested_q_{idx}"):
            handle_chat_submit(q)

    # ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
    _render_chat_messages()

    # ì±„íŒ… ì…ë ¥ ì¸í„°í˜ì´ìŠ¤
    render_chat_input(chat_manager)

    # AI ì‘ë‹µ ì²˜ë¦¬
    handle_ai_response(chat_manager)

    # --- ì±„íŒ… ì „ì²´ ë…¸ì…˜ ì €ì¥ ë²„íŠ¼ ---
    st.markdown("---")
    if st.button(LABELS["save_chat_notion"], key="save_chat_to_notion"):
        save_chat_to_notion()


def _initialize_chat_state():
    """ì±„íŒ… ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = [
            {
                "id": "welcome",
                "role": "assistant",
                "content": LABELS["chat_welcome"],
                "timestamp": datetime.datetime.now(),
            }
        ]
    if "chat_input" not in st.session_state:
        st.session_state.chat_input = ""
    if "chat_loading" not in st.session_state:
        st.session_state.chat_loading = False
    if "use_streaming" not in st.session_state:
        st.session_state.use_streaming = True


def _render_chat_settings():
    """ì±„íŒ… ì„¤ì • ì˜µì…˜ ë Œë”ë§"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        use_streaming = st.checkbox(
            LABELS["streaming_response"], value=st.session_state.use_streaming
        )
        st.session_state.use_streaming = use_streaming
    with col2:
        if st.button(LABELS["reset_chat"]):
            st.session_state.langchain_chat_manager.clear_memory()
            st.session_state.chat_messages = [
                {
                    "id": "welcome",
                    "role": "assistant",
                    "content": LABELS["reset_chat_msg"],
                    "timestamp": datetime.datetime.now(),
                }
            ]
            st.rerun()
    with col3:
        memory_info = len(st.session_state.langchain_chat_manager.memory.chat_memory.messages)
        st.info(LABELS["memory_info"].format(memory_info))


def _render_chat_messages():
    """ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥"""
    chat_container = st.container()
    with chat_container:
        for msg in st.session_state.chat_messages:
            align = "user" if msg["role"] == "user" else "assistant"
            with st.chat_message(align):
                st.markdown(msg["content"])


def render_chat_input(chat_manager: LangChainChatManager):
    """ì±„íŒ… ì…ë ¥ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
    # ì±„íŒ… ë¡œë”© ì¤‘ì´ë©´ ì…ë ¥ì°½ì„ ìˆ¨ê¹€
    if st.session_state.get("chat_loading", False):
        return

    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    col1, col2 = st.columns([8, 1])

    # ì…ë ¥ì°½ keyë¥¼ ì±„íŒ… ë©”ì‹œì§€ ê°œìˆ˜ë¡œ ë™ì ìœ¼ë¡œ ì§€ì •
    chat_input_key = f"chat_input_area_{len(st.session_state.get('chat_messages', []))}"

    with col1:
        user_input = st.text_area(
            LABELS["chat_input_placeholder"],
            value=st.session_state.chat_input,
            key=chat_input_key,
            label_visibility="collapsed",
            height=68,
            disabled=st.session_state.chat_loading,
            placeholder=LABELS["chat_input_placeholder"],
        )

    with col2:
        send_btn = st.button(
            LABELS["chat_send_btn"],
            key="chat_send_btn",
            disabled=not user_input.strip() or st.session_state.chat_loading,
            type="primary",
        )

    if user_input.strip() and not st.session_state.chat_loading:
        st.markdown(
            """
        <script>
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Enter' && e.ctrlKey) {
                document.querySelector('[data-testid="chat_send_btn"]').click();
            }
        });
        </script>
        """,
            unsafe_allow_html=True,
        )

    if send_btn and user_input.strip():
        handle_chat_submit(user_input.strip())


def handle_chat_submit(question: str):
    """ì±„íŒ… ì „ì†¡ ì²˜ë¦¬"""
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_messages.append(
        {
            "id": f"user-{uuid.uuid4()}",
            "role": "user",
            "content": question,
            "timestamp": datetime.datetime.now(),
        }
    )

    # ì…ë ¥ì°½ ì´ˆê¸°í™” ë° ë¡œë”© ìƒíƒœ ì„¤ì •
    st.session_state.chat_input = ""
    st.session_state.chat_loading = True
    st.rerun()


def handle_ai_response(chat_manager: LangChainChatManager):
    """AI ì‘ë‹µ ì²˜ë¦¬"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    if not st.session_state.chat_loading:
        return

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì‚¬ìš©ì ë©”ì‹œì§€ì¸ì§€ í™•ì¸
    if (
        len(st.session_state.chat_messages) < 1
        or st.session_state.chat_messages[-1]["role"] != "user"
    ):
        st.session_state.chat_loading = False
        return

    # í•„ìš”í•œ ë°ì´í„° í™•ì¸
    required_data = ["summary", "transcript_text", "selected_model_id", "model_provider"]
    missing_data = [key for key in required_data if key not in st.session_state]

    if missing_data:
        error_msg = LABELS["missing_data"].format(", ".join(missing_data))
        add_ai_message(error_msg)
        st.session_state.chat_loading = False
        st.session_state.chat_input = ""  # ì…ë ¥ì°½ ë¹„ìš°ê¸°
        st.rerun()
        return

    # API í‚¤ í™•ì¸
    model_id = st.session_state.selected_model_id
    provider = st.session_state.model_provider
    api_key_name = "gemini_api_key" if "gemini" in model_id else "openai_api_key"
    api_key = st.session_state.get(api_key_name)

    if not api_key:
        error_msg = LABELS["missing_api_key"].format(provider)
        add_ai_message(error_msg)
        st.session_state.chat_loading = False
        st.session_state.chat_input = ""  # ì…ë ¥ì°½ ë¹„ìš°ê¸°
        st.rerun()
        return

    # ì‘ë‹µ ìƒì„±
    question = st.session_state.chat_messages[-1]["content"]
    summary = st.session_state.summary
    transcript = st.session_state.transcript_text

    if st.session_state.use_streaming:
        handle_streaming_response(
            chat_manager, question, summary, transcript, model_id, provider, api_key
        )
    else:
        handle_regular_response(
            chat_manager, question, summary, transcript, model_id, provider, api_key
        )
    # ì‘ë‹µ í›„ ì…ë ¥ì°½ ë¹„ìš°ê¸°
    st.session_state.chat_input = ""


def handle_streaming_response(
    chat_manager: LangChainChatManager,
    question: str,
    summary: str,
    transcript: str,
    model_id: str,
    provider: str,
    api_key: str,
):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
    try:
        # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ placeholder ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in chat_manager.get_streaming_response(
                question, summary, transcript, model_id, provider, api_key
            ):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ
            message_placeholder.markdown(full_response)

        # ì„¸ì…˜ ìƒíƒœì— AI ì‘ë‹µ ì¶”ê°€
        add_ai_message(full_response)

    except Exception as e:
        error_msg = f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        add_ai_message(error_msg)

    finally:
        st.session_state.chat_loading = False
        st.rerun()


def handle_regular_response(
    chat_manager: LangChainChatManager,
    question: str,
    summary: str,
    transcript: str,
    model_id: str,
    provider: str,
    api_key: str,
):
    """ì¼ë°˜ ì‘ë‹µ ì²˜ë¦¬"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    try:
        with st.spinner(LABELS["chat_loading"]):
            response = chat_manager.get_response(
                question, summary, transcript, model_id, provider, api_key
            )

        add_ai_message(response)

    except Exception as e:
        error_msg = f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        add_ai_message(error_msg)

    finally:
        st.session_state.chat_loading = False
        st.rerun()


def add_ai_message(content: str):
    """AI ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€"""
    st.session_state.chat_messages.append(
        {
            "id": f"ai-{uuid.uuid4()}",
            "role": "assistant",
            "content": content,
            "timestamp": datetime.datetime.now(),
        }
    )


def save_chat_to_notion():
    """ì±„íŒ… ì „ì²´ë¥¼ Notionì— ì €ì¥"""
    from notion_utils import save_to_notion_as_page

    # ì±„íŒ… ë©”ì‹œì§€ í¬ë§·íŒ…
    chat_msgs = st.session_state.get("chat_messages", [])
    lines = []
    for msg in chat_msgs:
        role = "ğŸ™‹â€â™‚ï¸ ì‚¬ìš©ì" if msg["role"] == "user" else "ğŸ¤– AI"
        ts = msg["timestamp"].strftime("%Y-%m-%d %H:%M")
        lines.append(f"**{role}** ({ts})\n\n{msg['content']}\n")
    chat_md = "\n---\n".join(lines)

    # Save to Notion and handle success
    if save_to_notion_as_page(chat_md):
        st.success(LABELS["save_chat_notion_success"])
